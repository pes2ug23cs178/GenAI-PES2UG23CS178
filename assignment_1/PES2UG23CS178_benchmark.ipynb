{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "51d84bc9"
      },
      "outputs": [],
      "source": [
        "# Helper function for iterative text generation using a fill-mask pipeline\n",
        "def generate_text_with_mask(pipeline, prompt, mask_token, num_steps=5):\n",
        "    generated_text = prompt\n",
        "    for _ in range(num_steps):\n",
        "        text_with_mask = generated_text + \" \" + mask_token # Add space and mask token\n",
        "        predictions = pipeline(text_with_mask)\n",
        "        if not predictions:\n",
        "            break\n",
        "        # Take the top prediction token and append it\n",
        "        next_token = predictions[0]['token_str'].strip()\n",
        "\n",
        "        # Handle punctuation to avoid extra spaces\n",
        "        if next_token in [\" .\", \" ,\", \" !\", \" ?\", \" ;\"]: # RoBERTa might predict with leading space\n",
        "            generated_text += next_token.replace(\" \", \"\")\n",
        "        elif next_token in [\".\", \",\", \"!\", \"?\", \";\"]:\n",
        "            generated_text += next_token\n",
        "        elif generated_text.endswith(tuple([\" .\", \" ,\", \" !\", \" ?\", \" ;\"])):\n",
        "            # If previous token was punctuation, don't add space\n",
        "            generated_text += next_token\n",
        "        else:\n",
        "            generated_text += \" \" + next_token\n",
        "\n",
        "        # Prevent infinite loops if the model keeps predicting the same thing or padding tokens\n",
        "        if next_token == mask_token or next_token == \"[PAD]\" or next_token == \"</s>\":\n",
        "            break\n",
        "    return generated_text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh3Pdc57zVAT"
      },
      "source": [
        "## BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509,
          "referenced_widgets": [
            "1869259fd95a4a44b7b6b402810ffd38",
            "3d754d5ea7e24b5290aae3d3dc08cb0b",
            "b402d4e114284fd097a845f32f407168",
            "8601727c4e5b40df8fa24901a6f723f7",
            "f37a8c5ff6804385a529cc2eae8122e0",
            "1e31c024713647b8b82a0699883aaa52",
            "bf29ce1073ee4303a3e58add6d07b73a",
            "42b9729340414a3596659bd0c39dc31d",
            "286cb21385a045ec9058c8e6bf05a00e",
            "32fc1b17b85a46f5ac9985ef73ac19fc",
            "a0acaac149114776a5a3b2a4d25d7a61",
            "57967d8c411c419983c7f1b16cd5b831",
            "7dbdd4e7384b4ef086751cd7d7a65395",
            "9da0bccc243b4396a5ef16db43d8a2ef",
            "aeb6ef4a919b4c28b771767b31356d59",
            "73c65e12b177446a8e5bd39f6f48cb36",
            "e82ff8f6bd5a4da9afb3918e68463927",
            "1945bc671fac4b66a7977abe98ffe993",
            "76e9602307964541a688793cefe7289c",
            "22bbac98c6934932842756ab53ab3602",
            "556304c400c14e0d8a26a0adda874dd1",
            "0678c03199ac42e5a56f66086b5c7c0f",
            "1c8f329f5b8447aa84f18959be443e5a",
            "afeef74f9ec84fd7b213605426c060eb",
            "27408a83d1b14cf2970ed7e40ddcff32",
            "9e4c6d0f5bde49dabba68b4a1b875ba1",
            "0c6407741f8b4626ab0e32e7f8304c91",
            "462ac2e0eca34024a5ac83405fefa016",
            "7d2eddd1cfe74d78b14ee82a79652ff4",
            "bc86ac0fa2034b63b572da39494a5885",
            "7b382a6f0c4e48bfba28ac08c4cca49a",
            "2cc807f438db42fe898124d4bfb19223",
            "247bc37e72f5403c96fda08b408b2245",
            "9c66bc1c044840eabe16b561c0c0f924",
            "6c9ec5650cee41ab988fe727518c9a97",
            "09374e5ae0b64537a6634b46b25a823b",
            "a21f70a0a8364ad289c18c5b6ccff5f9",
            "6ff0708703ae4e7bbb2960963cb4e6e5",
            "74eec93c91ee4402b92b6b3634bcb2d3",
            "a4e9f76d815c4b8aaa6f3e7796cffb1a",
            "f050c6f552084cdb951b10699bc9fadd",
            "d178edf04217424685240cffe9abcd32",
            "f76d63d3bf494284a431fec450f1b7c8",
            "affaf3f2aae342f0b4698054633b4c8d",
            "709449a9468e432fb680e0fef1ffba07",
            "04afae0c6a464f95845b98ff481745d6",
            "75bfa054ac604ab7ab4f616f048794a5",
            "0fc28f2190e84ebeb30ea14cedf0145a",
            "dbad1342e1ed4934ac0c5e913dd6d17a",
            "cbb2d340f2114b9889de227aa9fa69e9",
            "5559c89d0e3a4a16ba5be67a346a4fac",
            "c33876126b644cc1aad077ec928fab0e",
            "8424e38ed576467fb9404fb3322fb92f",
            "e1b407cce97f432098d590982efec609",
            "bb7143058da24e32aff42b65c34c13fc"
          ]
        },
        "id": "b371946e",
        "outputId": "3d3f4b81-4ea4-4da9-9f21-d379d86edd04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1869259fd95a4a44b7b6b402810ffd38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57967d8c411c419983c7f1b16cd5b831",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c8f329f5b8447aa84f18959be443e5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c66bc1c044840eabe16b561c0c0f924",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer 'bert-base-uncased' loaded successfully.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "709449a9468e432fb680e0fef1ffba07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masked Language Model 'bert-base-uncased' loaded successfully.\n",
            "Question Answering Model 'bert-base-uncased' loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fill-mask pipeline created successfully.\n",
            "Question-answering pipeline created successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "# 2. Define the model name as 'bert-base-uncased'\n",
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "# 3. Load the tokenizer for the defined model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"Tokenizer '{model_name}' loaded successfully.\")\n",
        "\n",
        "# 4. Load the model for masked language modeling\n",
        "model_masked_lm = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "print(f\"Masked Language Model '{model_name}' loaded successfully.\")\n",
        "\n",
        "# 5. Load the model for question answering\n",
        "model_qa = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "print(f\"Question Answering Model '{model_name}' loaded successfully.\")\n",
        "\n",
        "# 6. Create a pipeline for 'fill-mask' tasks\n",
        "fill_mask_pipeline = pipeline(\"fill-mask\", model=model_masked_lm, tokenizer=tokenizer)\n",
        "print(\"Fill-mask pipeline created successfully.\")\n",
        "\n",
        "# 7. Create a pipeline for 'question-answering' tasks\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model_qa, tokenizer=tokenizer)\n",
        "print(\"Question-answering pipeline created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e84cc55c",
        "outputId": "65a200b9-6a05-4da0-8634-13ba076f77ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Performing Masked Prediction and Text Generation with BERT ---\n",
            "\n",
            "Generating text for: 'The future of Artificial Intelligence is'\n",
            "  Generated: 'The future of Artificial Intelligence is.....'\n",
            "\n",
            "Predicting for: 'The goal of Generative AI is to [MASK] new content.'\n",
            "  Token: create, Score: 0.5397\n",
            "  Token: generate, Score: 0.1558\n",
            "  Token: produce, Score: 0.0541\n",
            "  Token: develop, Score: 0.0445\n",
            "  Token: add, Score: 0.0176\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Performing Masked Prediction and Text Generation with BERT ---\")\n",
        "\n",
        "sentence1 = \"The future of Artificial Intelligence is\"\n",
        "print(f\"\\nGenerating text for: '{sentence1}'\")\n",
        "generated_text1 = generate_text_with_mask(fill_mask_pipeline, sentence1, '[MASK]')\n",
        "print(f\"  Generated: '{generated_text1}'\")\n",
        "\n",
        "sentence2 = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "print(f\"\\nPredicting for: '{sentence2}'\")\n",
        "predictions2 = fill_mask_pipeline(sentence2)\n",
        "for p in predictions2:\n",
        "    print(f\"  Token: {p['token_str']}, Score: {p['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDSTgJldzxPT"
      },
      "source": [
        "- BERT predicted the token like '.' indicating it recognises the need forending the sentence.\n",
        "- BERT predicted words like 'create', 'generate', and 'produce'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ea484e8",
        "outputId": "e976a6b5-2c63-4ffe-99ec-567be3e88c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Performing Question Answering ---\n",
            "\n",
            "Context: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'\n",
            "Question: 'What are the risks?'\n",
            "Answer: AI poses significant risks\n",
            "Score: 0.0048\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Performing Question Answering ---\")\n",
        "\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "print(f\"\\nContext: '{context}'\")\n",
        "print(f\"Question: '{question}'\")\n",
        "\n",
        "qa_result = qa_pipeline(question=question, context=context)\n",
        "print(f\"Answer: {qa_result['answer']}\")\n",
        "print(f\"Score: {qa_result['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vrJbvTV2HCS"
      },
      "source": [
        "BERT gets the context from the string that AI poses risks but does not mention what the risks are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NvwmFXI0Kuy"
      },
      "source": [
        "## RoBERTa model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "e49c3435aa7743e599c0459a2ad28741",
            "982e0d1ed2d14304b892f32280633309",
            "801ca9b01c0940d4931bda47bbab2af3",
            "277e4f5cbda04108b5cefdce555c6965",
            "a1dc135f6cc5487d96304b61e6d07770",
            "422274407de84c21a89fe8c7c03ad14a",
            "2edf63626a1f4c0980759891efe030d2",
            "f31929e2487f43b0b62d8fde646527c4",
            "6eb8a9c80bfc4d3a9bd2bc522d1cc503",
            "852f380188e54d3ab75147156a13b740",
            "8128307e285f46b2929ed409eb7779ec",
            "ea42232e499a461b89fb3266b8e1655d",
            "6a73f1b74d7d429a82a4c5e87bbd8b00",
            "b01fddeeea044bf9b1f9ea74c911acb2",
            "ce453ca72c41450e9eb5e85ee9b5385e",
            "9c91108b361f446fa8fa71095834e1a0",
            "b0e39529bced4c558d298a7d14797d26",
            "f6987f01f11843a28a46603f6c3a082b",
            "0ca5068e54f24652b6c9a089bca90021",
            "5b7b54ce29b74d0c89ec9d518adb74aa",
            "1774ef4a935344c69327b2c636d24e79",
            "6e969b2935d2452f9f0fd250fcd1366e",
            "bd420a88328e4d0c86ce759e402969ad",
            "996ba73a32424d0d9511ad85d2918cc4",
            "1646e2e4ae1d49fdb7ec54f4aa66c929",
            "80a711f589b9435cb3b29183537fd0cc",
            "cf8209fe7818420aa7fdeff69ea63409",
            "716840232caf4e998f11a1a8307789af",
            "d9419886cb3d4dcf8529dc5fe6ef284b",
            "3fe13be800c84f3fbe91717d3f81511f",
            "da076ad0f847414baeeae5ab5e88c2ba",
            "0b042cea74674725a4c32ca19a3c0073",
            "b3e923e4d71244deb4cd3615853c1c60",
            "93f1faba4e814a429f68697594d8cd81",
            "aef7474f8ffd45d6b1adc27135794b17",
            "e15bf6fdd69d44f7bebfed8d813f681f",
            "eb856d09643f4ceb9b8499be3cdd6cb1",
            "175510ba11014fe5ad50e397de424bda",
            "31dd9128e3a84aeca471e15a84e7b13c",
            "9f7c9591769b4acea6a37b1e97967816",
            "918264c4a2bc42f290ef0661ea208189",
            "9ece84abb8004f84a5fbe1e3dd0ad4b6",
            "bfec948df277431e85655f2a80e5c485",
            "00ab3daf966541828fd296b9ea5d0da1",
            "e8ee545b04534715b0b54e4e62d6d3a6",
            "1897d044d22740f290496444d533304d",
            "06799390b9eb4a3296a8eef3c75f5510",
            "22ac673b89a749abac729fe15e7dd3ea",
            "4eba97b4feb8421189951a2518b7ab66",
            "2f9f694dedb042229cb07ef65d7326d7",
            "d01466be6f7640438bcea6df9159b393",
            "ded7386549ba4f27b23f16ec90b6417d",
            "5c35772759c847839b4f5cb0f4e6a04f",
            "538be663d72841c09d064e7d8badf959",
            "99f34b254277429a8782d5bde285bf16",
            "4bb7684384704d61ac1fee5302efc1d2",
            "5dba8e0bb5d74d1a93052148e29caa86",
            "4ae68292e233471e85908e0ecb6cd015",
            "05fefaaa581c421f8fff7b3e0ace7b2d",
            "368e7d4f07d64b2f90b98502fa1b53eb",
            "ec7035baf5884455baa7d3cfb7fc6130",
            "96f222b7acc34519a6fb609b8af345ee",
            "00b90d7d8f894b358ee7d3ac78f051cb",
            "ea1920fc8e554bc09f03e3256c93741f",
            "e280b30ee9c943a192effb36f171ced0",
            "ea199883ae684d62b5610f9dac5c9ee6"
          ]
        },
        "id": "9e788a85",
        "outputId": "b1f10a74-9fd1-4c6a-ddfb-58f715a9e8a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e49c3435aa7743e599c0459a2ad28741",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea42232e499a461b89fb3266b8e1655d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd420a88328e4d0c86ce759e402969ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93f1faba4e814a429f68697594d8cd81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8ee545b04534715b0b54e4e62d6d3a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer 'roberta-base' loaded successfully.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bb7684384704d61ac1fee5302efc1d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masked Language Model 'roberta-base' loaded successfully.\n",
            "Question Answering Model 'roberta-base' loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa fill-mask pipeline created successfully.\n",
            "RoBERTa question-answering pipeline created successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "# Define the model name as 'roberta-base'\n",
        "model_name_roberta = 'roberta-base'\n",
        "\n",
        "# Load the tokenizer for the defined model\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(model_name_roberta)\n",
        "print(f\"Tokenizer '{model_name_roberta}' loaded successfully.\")\n",
        "\n",
        "# Load the model for masked language modeling\n",
        "model_masked_lm_roberta = AutoModelForMaskedLM.from_pretrained(model_name_roberta)\n",
        "print(f\"Masked Language Model '{model_name_roberta}' loaded successfully.\")\n",
        "\n",
        "# Load the model for question answering\n",
        "model_qa_roberta = AutoModelForQuestionAnswering.from_pretrained(model_name_roberta)\n",
        "print(f\"Question Answering Model '{model_name_roberta}' loaded successfully.\")\n",
        "\n",
        "# Create a pipeline for 'fill-mask' tasks\n",
        "fill_mask_pipeline_roberta = pipeline(\"fill-mask\", model=model_masked_lm_roberta, tokenizer=tokenizer_roberta)\n",
        "print(\"RoBERTa fill-mask pipeline created successfully.\")\n",
        "\n",
        "# Create a pipeline for 'question-answering' tasks\n",
        "qa_pipeline_roberta = pipeline(\"question-answering\", model=model_qa_roberta, tokenizer=tokenizer_roberta)\n",
        "print(\"RoBERTa question-answering pipeline created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29b81b3",
        "outputId": "25603cf1-1466-435a-8603-1f3d917f54b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Performing Masked Prediction and Text Generation with RoBERTa ---\n",
            "\n",
            "Generating text for: 'The future of Artificial Intelligence is'\n",
            "  Generated: 'The future of Artificial Intelligence is uncertain. </s>'\n",
            "\n",
            "Predicting for: 'The goal of Generative AI is to <mask> new content.'\n",
            "  Token:  generate, Score: 0.3711\n",
            "  Token:  create, Score: 0.3677\n",
            "  Token:  discover, Score: 0.0835\n",
            "  Token:  find, Score: 0.0213\n",
            "  Token:  provide, Score: 0.0165\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Performing Masked Prediction and Text Generation with RoBERTa ---\")\n",
        "\n",
        "sentence1_roberta = \"The future of Artificial Intelligence is\"\n",
        "print(f\"\\nGenerating text for: '{sentence1_roberta}'\")\n",
        "generated_text1_roberta = generate_text_with_mask(fill_mask_pipeline_roberta, sentence1_roberta, '<mask>')\n",
        "print(f\"  Generated: '{generated_text1_roberta}'\")\n",
        "\n",
        "sentence2_roberta = \"The goal of Generative AI is to <mask> new content.\"\n",
        "print(f\"\\nPredicting for: '{sentence2_roberta}'\")\n",
        "predictions2_roberta = fill_mask_pipeline_roberta(sentence2_roberta)\n",
        "for p in predictions2_roberta:\n",
        "    print(f\"  Token: {p['token_str']}, Score: {p['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d54aa52"
      },
      "source": [
        "- RoBERTa predicted the next token as 'uncertain', better than BERT predicting a string of '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53a3b7b4",
        "outputId": "5ed0a990-a3fb-415b-d974-d911bdbdb67e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Performing Question Answering with RoBERTa ---\n",
            "\n",
            "Context: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'\n",
            "Question: 'What are the risks?'\n",
            "Answer: as hallucinations, bias, and deepfakes.\n",
            "Score: 0.0052\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Performing Question Answering with RoBERTa ---\")\n",
        "\n",
        "context_roberta = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question_roberta = \"What are the risks?\"\n",
        "\n",
        "print(f\"\\nContext: '{context_roberta}'\")\n",
        "print(f\"Question: '{question_roberta}'\")\n",
        "\n",
        "qa_result_roberta = qa_pipeline_roberta(question=question_roberta, context=context_roberta)\n",
        "print(f\"Answer: {qa_result_roberta['answer']}\")\n",
        "print(f\"Score: {qa_result_roberta['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9QN-afS2zAh"
      },
      "source": [
        "RoBERTa extracted the right risks from the given context string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8963199e"
      },
      "source": [
        "## BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "b7dd04a959484dbf950196bdabe1ef75",
            "5e5f231ea10c4e0fa5ff487f475143ee",
            "0821fe7300f94661b0d6e17141d97f7f",
            "dc9bb9aeb6ed4b8c9ce58793e0f652ab",
            "3d17ea20e136461b9e4ec650165b5c9f",
            "5a66c71f578b4df0b970792425212291",
            "a4e4f5d8e68646eb88fb80630d48ff95",
            "2ef1f023c8b74299b60e2d116b024ad1",
            "5b67aba4292640c9b6e343b9fdda466c",
            "ccc61c4c2669491e805e6d2a69a14538",
            "2aea086743ba41b68f6e02c8e1a3b440",
            "f352dd975ea14b53a38b7eb67762bdff",
            "015f454ba6d04067af1e6e44cafa5d31",
            "f1a51312a23d4f4eaa9c1d23824db811",
            "02b66278008842989c8861c12c39de77",
            "fe58766ddf8d44f3b9eaee2366cff7a7",
            "23e6646c45da48b19ecf2dc4242f65f0",
            "37926500354942869f1006095a751023",
            "d0c52b1c933442b6a3900c2a386c88a0",
            "668a4462f2134026b7cc2940f4b99e9a",
            "d785265a82864d6493a9e9b7d86c1cce",
            "f94ab2ed920b42d398dc3e9aeb00a12b",
            "8bbc6db1dc4f4c6c87a34b3ec995fdb2",
            "91691d46d32a44f3a0fcf56bb5cc6242",
            "a033bd51cebc48d2a59abc7d89c12416",
            "3014a6f8efcf44159d67886e17e4cfa7",
            "18cd737423c94360a948a09961ee2e70",
            "d0642cbf593041aaa7cda6c367cd3ca9",
            "44fb03ad4ecc4989ad633731f4afaafb",
            "7c4ca2e1beab4cc7bba6b6e58feebe3e",
            "2eaabd25222e4f019efc9f44b337fb09",
            "1679f77fb8b8483891031b9b594b7a2e",
            "98a8541cc89846cf846d49304f9d9c1f",
            "d36645e719d64ac39402af5575aeba30",
            "b1350ad4c83c4ce1a2abebdd0b9d7d7c",
            "e9336328924346e7b16f0f23f051c5b8",
            "6841cd3decd74ee8b16d95c204977f0f",
            "444727d5cb034abb9223502451f7199b",
            "ca0dbad50774486d941a8d1e179a856f",
            "39b7a31f78b147239f39e2ffa2fba000",
            "a38c5096db30499f87e0a1dd502de379",
            "86b2993dd719481787a8a8426599dcea",
            "1e80ab2cdcf44f3cb4a87de72218e091",
            "b996f65a2c2f45388a30a59bd8ac2f64",
            "c3adc8a1d09b4216aa4fc4c98a64c16e",
            "ee56452fcdbf4b46a99e716334a949cf",
            "562087d9deed46de941a4378b25ea006",
            "0ce6edd326ad4d52bca5f24d238b139b",
            "b73194959ac84af5b01c91e3e51caa0f",
            "1df7a27193b94ad48ea8044bf5f6f770",
            "bf4bbe4744484b0e8cb810b9baf19079",
            "890ef6479d314fdbb174ef09adac1928",
            "8ec14e5d740745e68dd50d6fca4eb802",
            "55f6a05265bf41ad83416cb1dfc08809",
            "2b2dbf8c527f4b5a930f60d42060553a"
          ]
        },
        "id": "c9e5cc31",
        "outputId": "5f08fab3-2627-4130-e946-804127af6114"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7dd04a959484dbf950196bdabe1ef75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f352dd975ea14b53a38b7eb67762bdff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bbc6db1dc4f4c6c87a34b3ec995fdb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d36645e719d64ac39402af5575aeba30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer 'facebook/bart-base' loaded successfully.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3adc8a1d09b4216aa4fc4c98a64c16e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masked Language Model 'facebook/bart-base' loaded successfully.\n",
            "Question Answering Model 'facebook/bart-base' loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BART fill-mask pipeline created successfully.\n",
            "BART question-answering pipeline created successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "# Define the model name as 'facebook/bart-base'\n",
        "model_name_bart = 'facebook/bart-base'\n",
        "\n",
        "# Load the tokenizer for the defined model\n",
        "tokenizer_bart = AutoTokenizer.from_pretrained(model_name_bart)\n",
        "print(f\"Tokenizer '{model_name_bart}' loaded successfully.\")\n",
        "\n",
        "# Load the model for masked language modeling\n",
        "model_masked_lm_bart = AutoModelForMaskedLM.from_pretrained(model_name_bart)\n",
        "print(f\"Masked Language Model '{model_name_bart}' loaded successfully.\")\n",
        "\n",
        "# Load the model for question answering\n",
        "model_qa_bart = AutoModelForQuestionAnswering.from_pretrained(model_name_bart)\n",
        "print(f\"Question Answering Model '{model_name_bart}' loaded successfully.\")\n",
        "\n",
        "# Create a pipeline for 'fill-mask' tasks\n",
        "fill_mask_pipeline_bart = pipeline(\"fill-mask\", model=model_masked_lm_bart, tokenizer=tokenizer_bart)\n",
        "print(\"BART fill-mask pipeline created successfully.\")\n",
        "\n",
        "# Create a pipeline for 'question-answering' tasks\n",
        "qa_pipeline_bart = pipeline(\"question-answering\", model=model_qa_bart, tokenizer=tokenizer_bart)\n",
        "print(\"BART question-answering pipeline created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf2f6b5",
        "outputId": "5065b870-1a2d-45f5-d2a2-3c82bcca2d09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Performing Masked Prediction and Text Generation with BART ---\n",
            "\n",
            "Generating text for: 'The future of Artificial Intelligence is'\n",
            "  Generated: 'The future of Artificial Intelligence is in the hands of the people. advertisement : advertisement'\n",
            "\n",
            "Predicting for: 'The goal of Generative AI is to <mask> new content.'\n",
            "  Token:  create, Score: 0.0746\n",
            "  Token:  help, Score: 0.0657\n",
            "  Token:  provide, Score: 0.0609\n",
            "  Token:  enable, Score: 0.0359\n",
            "  Token:  improve, Score: 0.0332\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Performing Masked Prediction and Text Generation with BART ---\")\n",
        "\n",
        "sentence1_bart = \"The future of Artificial Intelligence is\"\n",
        "print(f\"\\nGenerating text for: '{sentence1_bart}'\")\n",
        "generated_text1_bart = generate_text_with_mask(fill_mask_pipeline_bart, sentence1_bart, '<mask>', 10)\n",
        "print(f\"  Generated: '{generated_text1_bart}'\")\n",
        "\n",
        "sentence2_bart = \"The goal of Generative AI is to <mask> new content.\"\n",
        "print(f\"\\nPredicting for: '{sentence2_bart}'\")\n",
        "predictions2_bart = fill_mask_pipeline_bart(sentence2_bart)\n",
        "for p in predictions2_bart:\n",
        "    print(f\"  Token: {p['token_str']}, Score: {p['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac77c9d",
        "outputId": "2fa24b12-846b-4e12-f99e-3d4883659d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Performing Question Answering with BART ---\n",
            "\n",
            "Context: 'Generative AI poses significant risks such as hallucinations, bias, and deepfakes.'\n",
            "Question: 'What are the risks?'\n",
            "Answer: poses significant risks such\n",
            "Score: 0.0293\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Performing Question Answering with BART ---\")\n",
        "\n",
        "context_bart = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question_bart = \"What are the risks?\"\n",
        "\n",
        "print(f\"\\nContext: '{context_bart}'\")\n",
        "print(f\"Question: '{question_bart}'\")\n",
        "\n",
        "qa_result_bart = qa_pipeline_bart(question=question_bart, context=context_bart)\n",
        "print(f\"Answer: {qa_result_bart['answer']}\")\n",
        "print(f\"Score: {qa_result_bart['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0e02cd0"
      },
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | *Failure* | Generated '.....' (primarily punctuation). | BERT is an Encoder-only model, not designed for open-ended text generation. |\n",
        "| | RoBERTa | *Success* | Generated 'uncertain. </s>' | RoBERTa is also Encoder-only but its training leads to more plausible iterative masked predictions. |\n",
        "| | BART | *Partial Success* | Generated 'in the hands of the people.<br> advertisement : advertisement'.<br>More coherent than BERT, but still incomplete. | BART is an Encoder-Decoder model, designed for text generation, but `num_steps` limit and general training affect coherence. |\n",
        "| **Fill-Mask** | BERT | *Success* | Predicted 'create', 'generate'. | BERT is explicitly trained on Masked Language Modeling (MLM). |\n",
        "| | RoBERTa | *Success* | Predicted 'generate', 'create'. | RoBERTa is also explicitly trained on Masked Language Modeling (MLM). |\n",
        "| | BART | *Partial Success* | Predicted 'create', 'help', 'provide'.<br>Lower scores and less focused top predictions. | BART uses masked inputs during pre-training, but its Encoder-Decoder architecture can influence prediction distribution. |\n",
        "| **QA** | BERT | *Partial Success* | Answered 'AI poses significant risks' (Score: 0.0048). Did not list specific risks. | BERT-based QA extracts a span. It found a relevant, but incomplete, general span. |\n",
        "| | RoBERTa | *Success* | Answered 'as hallucinations, bias, and deepfakes.'<br> (Score: 0.0052). Clearly listed specific risks. | RoBERTa-based QA extracts spans. It successfully identified the precise span containing the risks. |\n",
        "| | BART | *Partial Success* | Answered 'poses significant risks such' (Score: 0.0293).<br>More specific than BERT, but incomplete compared to RoBERTa. | BART performs QA by span extraction/generation. It extracted a relevant, but incomplete, leading phrase. |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
